<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Adaptor: Assistive Teleoperation</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400;500;700&family=Inter:wght@300;400;600&family=Work+Sans:ital,wght@1,800;1,900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="styles.css">
</head>
<body>

    <nav class="nav-pill">
        <div class="nav-inner">
            <span class="logo">Adaptor</span>
            <div class="nav-links">
                <a href="#abstract">Abstract</a>
                <a href="#motivation">Motivation</a>
                <a href="#method">Method</a>
                <a href="#experiments">Experiments</a>
                <a href="#results">Results</a>
                <a href="#bibtex">BibTeX</a>
            </div>
            <div class="nav-icons">
                <a href="#" class="icon-link"><i class="fa-solid fa-file-pdf"></i></a>
                <a href="#" class="icon-link"><i class="fa-brands fa-github"></i></a>
            </div>
        </div>
    </nav>

    <main class="standard-container" style="padding-bottom: 40px;">
        
        <section class="hero">
            <div style="height: 50px;"></div>
            <h1 class="paper-title">
                <span class="title-part highlight-text">Adaptor</span>: Advancing Assistive Teleoperation <br>
                with Few-Shot Learning <br>
                and Cross-Operator Generalization
            </h1>
            
            <div class="authors">
                <a href="#" class="author">Yu Liu<sup>1</sup></a>,
                <a href="#" class="author">Yihang Yin<sup>2</sup></a>,
                <a href="#" class="author">Tianlv Huang<sup>1</sup></a>,
                <a href="#" class="author">Fei Yan<sup>1</sup></a>,
                <a href="#" class="author">Yuan Xu<sup>1</sup></a>,
                <a href="#" class="author">Weinan Hong<sup>1</sup></a>,<br>
                <a href="#" class="author">Wei Han<sup>1</sup></a>,
                <a href="#" class="author">Yue Cao<sup>2</sup></a>,
                <a href="#" class="author">Xiangyu Chen<sup>2</sup></a>,
                <a href="#" class="author">Zipei Fan<sup>1,†</sup></a>,
                <a href="#" class="author">Xuan Song<sup>1</sup></a>
            </div>
            
            <div class="affiliations">
                <p><sup>1</sup>School of Artificial Intelligence, Jilin University &nbsp;&nbsp; <sup>2</sup>IO-AI TECH</p>
                <p class="email">† Corresponding author: fanzipei@jlu.edu.cn</p>
            </div>

            <div class="buttons">
                <a href="#" class="btn btn-primary"><i class="fa-solid fa-file-pdf"></i> Paper</a>
                <a href="#" class="btn btn-dark"><i class="fa-brands fa-github"></i> Code</a>
                <a href="#" class="btn btn-secondary"><i class="fa-solid fa-database"></i> Data</a>
            </div>
        </section>

        <section class="teaser">
            <div class="teaser-grid">
                <div class="grid-item"><video autoplay muted loop playsinline src="videos/1-1.mp4"></video></div>
                <div class="grid-item"><video autoplay muted loop playsinline src="videos/2-1.mp4"></video></div>
                <div class="grid-item"><video autoplay muted loop playsinline src="videos/3-1.mp4"></video></div>
                <div class="grid-item"><video autoplay muted loop playsinline src="videos/4-1.mp4"></video></div>
                <div class="grid-item"><video autoplay muted loop playsinline src="videos/5-1.mp4"></video></div>
                <div class="grid-item"><video autoplay muted loop playsinline src="videos/6-1.mp4"></video></div>
                <div class="grid-item"><video autoplay muted loop playsinline src="videos/7-1.mp4"></video></div>
                <div class="grid-item"><video autoplay muted loop playsinline src="videos/8-1.mp4"></video></div>
                <div class="grid-item"><video autoplay muted loop playsinline src="videos/9-1.mp4"></video></div>
            </div>
            <p class="caption">
                <strong>Adaptor</strong> generalizes intent recognition across operators with varying expertise levels and habits via few-shot learning.
            </p>
        </section>

        <section id="abstract" class="section" style="margin-bottom: 0;">
            <h2>Abstract</h2>
            <p class="text-block">
                Assistive teleoperation improves efficiency by adopting a shared-control paradigm where humans provide high-level guidance and robots execute low-level actions. However, inter-operator variability stemming from diverse habits and expertise results in highly heterogeneous trajectory distributions, challenging the stability of intent recognition. To address this, we introduce <span class="highlight-chip">Adaptor</span>, a framework designed to generalize intent recognition via few-shot learning. Adaptor operates in two core phases: in preprocessing, it models intent uncertainty by constructing trajectory perturbation distributions via noise injection and employing geometry-aware keyframe extraction; in policy learning, these processed trajectories are encoded by an Intention Expert and fused with pre-trained VLM features to condition an Action Expert for precise generation. We evaluate Adaptor on six manipulation tasks across simulation and real-world setups with users of varying experience. Results demonstrate that Adaptor significantly outperforms direct teleoperation and prior assistive methods in success rate, completion time, and user satisfaction, achieving state-of-the-art performance.
            </p>
        </section>
    </main>

    <div class="nerfies-bg" id="motivation-teaser">
        <div class="wide-container">
            <div class="nerfies-gallery">
                <div class="nerfies-item">
                    <video autoplay muted loop playsinline src="videos/depth.mp4"></video>
                    <div class="nerfies-label worksans-text">Depth<br>Uncertainty</div>
                </div>
                <div class="nerfies-item">
                    <video autoplay muted loop playsinline src="videos/kinematics.mp4"></video>
                    <div class="nerfies-label worksans-text">Kinematics<br>Mismatch</div>
                </div>
                <div class="nerfies-item">
                    <video autoplay muted loop playsinline src="videos/collision.mp4"></video>
                    <div class="nerfies-label worksans-text">Joint<br>Collision</div>
                </div>
                <div class="nerfies-item">
                    <video autoplay muted loop playsinline src="videos/workload.mp4"></video>
                    <div class="nerfies-label worksans-text">High<br>Workload</div>
                </div>
            </div>
        </div>
    </div>

    <main class="standard-container">
        
        <section id="motivation" class="section">
            <h2 class="section-title-left">Motivation</h2>
            <p class="text-block">
                Inter-operator heterogeneity presents a challenge for learning-based teleoperation. Operators with varying levels of experience demonstrate significant differences in trajectory smoothness and operational speed, resulting in distribution shifts that degrade the performance of policies trained solely on expert data.<br><br>
                Existing methods, such as Behavior Cloning, tend to overfit to specific expert demonstrations, leading to failures when encountering unseen user habits. Furthermore, scaling data collection to cover all potential operator styles is impractical due to high costs. Adaptor addresses these issues by modeling intent uncertainty, enabling generalization to diverse operators without requiring exhaustive data coverage.
            </p>
        </section>

        <section id="method" class="section">
            <h2 class="section-title-left">Method</h2> <p class="section-desc">
                The Adaptor framework consists of two phases designed to handle intent variability.<br><br>
                <strong>Phase I: Intention Preprocessing.</strong> We construct a perturbation distribution by injecting noise into expert trajectories to simulate suboptimal behaviors. Additionally, a geometry-aware keyframe extraction method is applied to retain critical temporal information while reducing redundancy.<br><br>
                <strong>Phase II: VLA-Based Policy Learning.</strong> Processed trajectories are encoded by an Intention Expert and combined with context from a pre-trained Vision-Language Model (VLM). This conditions the Action Expert, which uses Flow Matching to generate control commands.
            </p>
            <div class="image-container zoom-effect">
                <img src="imgs/framework.png" alt="Adaptor Framework Overview" class="responsive-img"> 
            </div>
        </section>

        <section id="experiments" class="section">
            <h2 class="section-title-left">Experiments</h2>
            <p class="section-desc">
                We evaluate Adaptor on six manipulation tasks across three robotic platforms.
            </p>
            <div class="video-custom-container" onclick="toggleVideo(this)">
                <video class="main-video" poster="imgs/video_cover.jpg" playsinline>
                    <source src="videos/experiments.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <div class="play-overlay">
                    <div class="play-btn-circle">
                        <i class="fa-solid fa-play"></i>
                    </div>
                </div>
            </div>
        </section>

        <section id="results" class="section">
            <h2 class="section-title-left">Results</h2>
            <div class="results-grid">
                <div class="result-card">
                    <div class="result-value green">+41.9%</div>
                    <div class="result-label">Success Rate</div>
                    <div class="result-sub">Improvement vs. Baselines</div>
                </div>
                <div class="result-card">
                    <div class="result-value blue">-32.2%</div>
                    <div class="result-label">Completion Time</div>
                    <div class="result-sub">Efficiency Gain</div>
                </div>
                <div class="result-card">
                    <div class="result-value orange">83.2%</div>
                    <div class="result-label">Novice Success</div>
                    <div class="result-sub">Few-Shot Generalization</div>
                </div>
                <div class="result-card">
                    <div class="result-value purple">9/10</div>
                    <div class="result-label">User Satisfaction</div>
                    <div class="result-sub">Top Metrics Score</div>
                </div>
            </div>
        </section>

        <section id="bibtex" class="section">
            <h2 class="section-title-left">BibTeX</h2>
            <div class="code-block">
<pre><code>@article{liu2026adaptor,
  title={Adaptor: Advancing Assistive Teleoperation with Few-Shot Learning and Cross-Operator Generalization},
  author={Liu, Yu and others},
  journal={arXiv},
  year={2026}
}</code></pre>
            </div>
        </section>
        
        <footer class="footer">
            <p>Project page template inspired by <a href="https://github.com/InternRobotics/internvla-a1.github.io">InternVLA</a> and <a href="https://nerfies.github.io/">Nerfies</a>.</p>
        </footer>

    </main>

    <script>
        function toggleVideo(container) {
            const video = container.querySelector('.main-video');
            const overlay = container.querySelector('.play-overlay');
            if (video.paused) {
                video.play();
                video.controls = true;
                overlay.style.opacity = '0';
                overlay.style.pointerEvents = 'none';
            } else {
                video.pause();
                video.controls = false;
                overlay.style.opacity = '1';
                overlay.style.pointerEvents = 'auto';
            }
        }
    </script>
</body>
</html>